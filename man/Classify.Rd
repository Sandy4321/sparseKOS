% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Wrappers.R
\name{Classify}
\alias{Classify}
\title{Classifies data.}
\usage{
Classify(X, Data, Cat, Sigma = NULL, Gamma = NULL, Lambda = NULL)
}
\arguments{
\item{X}{(m x p) Matrix of unlabelled data with numeric features to be classified. Cannot have missing values.}

\item{Data}{(n x p) Matrix of training data with numeric features. Cannot have missing values.}

\item{Cat}{(n x 1) Vector of class membership corresponding to Data. Values must be either 1 or 2.}

\item{Sigma}{Scalar Gaussian kernel parameter. Default set to NULL and is automatically generated by the function.. Must be > 0.}

\item{Gamma}{Scalar ridge parameter used in kernel optimal scoring. Default set to NULL and is automatically generated by the function. Must be > 0.}

\item{Lambda}{Scalar sparsity parameter on weight vector. Default set to NULL and is automatically generated by the function. Must be >= 0. When Lambda = 0, SparseKOS defaults to kernel optimal scoring of [Lapanowski and Gaynanova, preprint] without sparse feature selection.}
}
\value{
\item{Predictions}{ (m x 1) Vector of predicted class labels for the data points in Data.}
}
\description{
Returns a (m x 1) vector of predicted group membership (either 1 or 2) for each data point in X. Uses Data and Cat to train the classifier.
}
\details{
Classifies each data point in X. Will use user-supplied parameters Sigma, Gamma, and Lambda if all are given. If any are missing, the function will run SelectParams to autogenerate parameters.
}
\examples{
Sigma <- 1.325386  #Set parameter values equal to result of SelectParam.
Gamma <- 0.07531579 #Speeds up example.
Lambda <- 0.002855275
Classify(X = Data$TestData , 
         Data = Data$TrainData , 
         Cat = Data$CatTrain , 
         Sigma = Sigma , 
         Gamma = Gamma , 
         Lambda = Lambda)
Classify(X = Data$TestData , 
         Data = Data$TrainData , 
         Cat = Data$CatTrain)
}
\references{
Lapanowski, Alexander F., and Gaynanova, Irina. ``Sparse feature selection in kernel discriminant analysis via optimal scoring'', preprint.
}
